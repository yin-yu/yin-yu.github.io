
<head>
  <style>

    .round_icon{
/*      width: 200px;
      height: 200px;*/
      width:"15%";
      height: 200px;
      display: flex;
      border-radius: 50%;
      align-items: center;
      justify-content: center;
      overflow: hidden;
    }
      
  </style>
  
    <meta charset="utf-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Yu Yin's Homepage</title>
<!--     <link rel="icon" href="./doc/icon/icon.png"> -->
    
    <link rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/3.3.7/css/bootstrap.min.css">
    <script src="https://cdn.staticfile.org/jquery/2.1.1/jquery.min.js"></script>
    <script src="https://cdn.staticfile.org/twitter-bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!--<link rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/3.3.7/css/bootstrap.min.css">-->

</head>

 
<body>
  
  
  <script src="https://code.jquery.com/jquery.js"></script>
  <script src="js/bootstrap.min.js"></script>
    
    <div style="max-width:800px" class="container">

    <div id="header">
    <div class="spacer"></div>

    <!-- <h1><strong style="font-family:'Comic Sans MS'; font-size:24pt; "><div align="center">  Welcome to Yu Yin's Homepage</strong> </h1> -->
    <h1><strong style="font-family:'Times New Roman'; font-size:24pt; "><div align="center">  Welcome to Yu Yin's Homepage</strong> </h1>
      

    <br>
    <table width="712" border="0" align="left" cellspacing="3" bordercolor="#999999">
      <tbody><tr bordercolor="#333333">
        <!-- <th width="19%" height="30" scope="col"><a href="https://yin-yu.github.io/#biography" class="STYLE212">Biography</a></th> -->
        <th width="19%" scope="col"><div align="center"><span class="STYLE217" style="font-size: 18"><b style="font-family:'Times New Roman'; mso-bidi-font-weight: normal"><a href="https://yin-yu.github.io/#biography" class="STYLE212">Biography</a></b></span></div></th>
        <th width="20%" scope="col"><div align="center"><span class="STYLE217" style="font-size: 18"><b style="font-family:'Times New Roman'; mso-bidi-font-weight: normal"><a href="https://yin-yu.github.io/#Education">Education</a></b></span></div></th>
        <th width="22%" scope="col"><div align="center"><span class="STYLE217" style="font-size: 18"><b style="font-family:'Times New Roman'; mso-bidi-font-weight: normal"><a href="https://yin-yu.github.io/#Publications">Publications</a></b></span></div></th>
        <th width="20%" scope="col"><div align="center"><span class="STYLE217" style="font-size: 18"><b style="font-family:'Times New Roman'; mso-bidi-font-weight: normal"><a href="https://yin-yu.github.io/#Activities">Activities</a></b></span></div></th>
        <th width="19%" scope="col"><div align="center"><span class="STYLE217" style="font-size: 18"><b style="font-family:'Times New Roman'; mso-bidi-font-weight: normal"><a href="https://yin-yu.github.io/#Awards">Awards</a></b></span></div></th>
      </tr>
    </tbody></table>
    <p><br>
    <hr/>

    <br>
    <table cellpadding="20" style="font-size: 17px">
    <tr>

    <td width="25%" height="215"><div align="center" class="STYLE90" src="">
    <div align="center"><a href="https://yin-yu.github.io/"><img src="img/yuyin.jpg" alt="Yu Yin" width="176" height="221" hspace="20" vspace="0" border="2" class="STYLE35"></a></div>
    </div></td>
    
    <td width="10%"></td>
    
    <td>  
    <h1><strong style="font-family: 'Times New Roman';">Yu Yin  &nbsp</strong> <strong style="font-family:STFangsong; font-size:30pt; "> (印宇)</strong></h1>
    <br>

  <I>Ph.D. candidate </I><br>
  <I>Department of ECE, College of Engineering<a href="https://ece.northeastern.edu/" class="STYLE189"> </I><br>
    <I>Northeastern University, Boston, USA </I><br> <br>
    
    <a class="email" href="mailto:yin.yu1@northeastern.edu" style="color: #333;">
      <i class="fa fa-envelope" aria-hidden="true"></i>
      &nbsp;&nbsp; Email (yin.yu1[at]northeastern.edu)
    </a><br>
    </center>

    <a class="menu" href="https://github.com/YuYin1" style="color: #333;">
      <i class="fa fa-github" aria-hidden="true"></i>
      &nbsp;&nbsp; GitHub
    </a><br>
    
    <a class="menu" href="https://www.linkedin.com/in/yu-yin/" style="color: #333;">
      <i class="fa fa-linkedin" aria-hidden="true"></i>
      &nbsp;&nbsp; Linkedin
    </a>
      
    </td>
    </tr>   
    </table>
  </div> <!-- end header -->

<div id="main">
<div id="maincontent">

<div class="content">
  <h4 class="text-primary" id="news"><font color = 'red';><I>What's New</I></font></h4>
    <!-- <a href="https://"> CVPR </a> --> 
    I accepted the invitation to serve as Program Committee member of IEEE MIPR 2019.

I will be joining the Department of Computer Science at University of Georgia as a Tenure-Track Assistant Professor from Fall 2018.

    [2023.05] <font color = 'red';> I will be joining the Department of Computer and Data Science at <strong>Case Western Reserve University</strong> (<a href="https://engineering.case.edu/computer-and-data-sciences">CWRU</a>) as a Tenure-Track Assistant Professor from Fall 2023!</font></a><br>
    [2023.04] We will organize The 11th IEEE International Workshop on Analysis and Modeling of Faces and Gestures (<a href="https://web.northeastern.edu/smilelab/amfg2023/">AMFG</a>) at ICCV 2023. <br> 
    [2023.04] I received the Dissertation Completion Fellowship from Northeastern University.</a><br>
    [2023.02] One paper is accepted by <a href="https://cvpr.thecvf.com/"> CVPR 2023</a>. The website of this work is on-line at <a href="https://yuyin1.github.io/NeRFInvertor_Homepage/">"NeRFInvertor".</a><br>
    [2022.12] Two papers are accepted by IEEE Trans. on Image Processing (TIP).<br>
    [2022.07] One paper is accepted by <a href="https://2022.acmmm.org/"> ACM MM 2022 </a>.<br>
    [2022.03] One paper is accepted by <a href="https://www.icmr2022.org/"> ACM ICMR 2022 </a>.<br>
    [2021.12] Our paper is accepted by <a href="https://www.siam.org/conferences/cm/conference/sdm22"> SDM 2022 </a>.<br>
    [2021.07] One paper is accepted by <a href="https://2021.acmmm.org/"> ACM MM 2021 </a>.<br>
    [2020.12] Our paper is accepted by <a href="https://www.siam.org/conferences/cm/conference/sdm21"> SDM 2021 </a>.<br>
    [2020.05] Our paper is accepted by <a href="https://www.computer.org/csdl/proceedings/icdm/2020/1r54vmgaSyY"> ICDM 2020 </a>.<br>
    [2020.03] We release the code for DA-GAN (accepted by FG 2020) at <a href="https://github.com/YuYin1/DA-GAN">Github_DA-GAN</a>.<br>
    [2020.03] Our paper is accepted by <a href="https://www.computer.org/csdl/proceedings/fg/2020/1quywnlzjdS"> FG 2020 </a>.<br>
    [2019.12] We release the code for JASRNet (accepted by AAAI 2020) at <a href="https://github.com/YuYin1/JASRNet">Github_JASRNet</a>.<br>
    [2019.11] Our paper is accepted by <a href="https://aaai.org/conference/aaai/aaai-20/"> AAAI 2020 </a>.<br>
    [2019.08] Our paper is accepted by Expert Systems With Applications (IF: 4.292).<br>
    [2019.02] Our paper is accepted by IEEE Journal of Translational Engineering in Healthand Medicine (JTEHM).<br>
    [2018.10] Our paper is accepted by IEEE Journal of Translational Engineering in Healthand Medicine (JTEHM).<br>
    [2018.06] Our paper is accepted by IJCAI Workshop 2018.<br>
    [2017.10] Our paper is accepted by IEEE-NIH Special Topics Conference on Healthcare Innovations and Point-of-Care Technologies (HI-POCT'17).<br>
    <!-- <p>[July 2019] <span class="glyphicon glyphicon-plane"></span>&nbsp;  Travel to Florence, Italy - ACL 2019 <br> <p> -->

  <br><font color = 'red';><strong>Openings: </strong> I am continuously looking for highly-motivated Ph.D. students to work on machine learning, computer vision and causal inference. Please send me your CV if interested. </font><br> 

  <!-- <br><font color = 'red';><I>I am looking for talented graduate students starting from 2023 Fall or 2024 Sping:</I> </font> The lab is a fast-paced environment and there will be many challenging problems across a variety of disciplines. You will have to work hard, but in return you will have many opportunities to shape our research and pursue your unique take on our problems. I am especially interested in PhD students with a background in machine learning, computer vision, and algorithms. If this sounds like the kind of place you love to work, send me an email with your updated CV.<br>    -->
<hr />
</div>

<!--Biography-->
<h2 id="Biography" class="top_main_heading"><span style=:'Times New Roman';">Biography</span></h2>
    <p>
        I am a fifth-year Ph.D. student at Department of Electrical & Computer Engineering, <a href="https://www.northeastern.edu/">Northeastern University</a>, and work with Prof. <a href="http://www1.ece.neu.edu/~yunfu/">Yun (Raymond) Fu</a> in the <a href="https://web.northeastern.edu/smilelab/">SMILE</a> Lab. Before that I received my master degree in Electrical & Computer Engineering from Northeastern University in Dec. 2018, and B.E degree from School of Electronic Engineering, <a href="http://english.whut.edu.cn/">Wuhan University of Technology</a>, China, in Jul. 2016. From Fall 2016, I was a member of the <a href="https://web.northeastern.edu/ostadabbas/">Augmented Cogniton (AClab)</a>, under the supervision of Prof. <a href="https://web.northeastern.edu/ostadabbas/teams/sarah-ostadabbas/">Sarah Ostadabbas</a>.
        My research interest broadly includes visual synthesis and understanding, multi-modality fusion, and transfer learning. My recent research topics lie in unified representation learning, 3d modeling and animation, and a wide range of computer vision applications, including image synthesis, 3D novel-view synthesis, face restoration, forensics detection, 3D model generation and rendering, etc. 
        <!-- My research interest broadly includes image processing (i.e., super-resolution, face generation), visual recognition (i.e., face recognition, pose estimation, face alignment, and emotion recognition), and biosignal processing. <br> <br> -->
        <!-- I was the recipient of the Best Student Paper Award at IEEE International Conference on Visual Communication and Image Processing (VCIP) in 2015. -->
        <hr/>
    </p>


<!--Education-->
<h2 id="Education" class="top_main_heading"><span style=:'Times New Roman';">Education</span></h2>
  <ul>
    <li>
      01/2019 - Now: Ph.D. in Computer Engineering, <a href="https://www.northeastern.edu/">Northeastern University</a>, Boston, USA
    </li>
    <li>
      09/2016 - 12/2018: M.S. in Electrical and Computer Engineering, <a href="https://www.northeastern.edu/">Northeastern University</a>, Boston, USA
    </li>
    <li>
      09/2012 - 07/2016: B.E. in Electrical and Information Engineering, <a href="http://english.whut.edu.cn/">Wuhan University of Technology</a>, Wuhan, China
    </li>
  </ul>
<hr />


<!--Teaching-->
<h2 id="Teaching" class="top_main_heading"><span style=:'Times New Roman';">Teaching</span></h2>
  <ul> 
    <li><strong>Serve as Instructor</strong>
    <ul> 
      <li>Instructor of <a href="./paper/teaching/syllabus-Spring2021.html">Data Visualization</a> course (EECE 5642), Northeastern University, USA, 2021 Spring</li>

    </ul>
  </ul> 
  <ul> 
    <li><strong>Serve as Teaching Assistance</strong>
    <ul> 
      <li>TA of EECE 5642 - Data visualization, Northeastern University, 2022 & 2023 Spring</li>
      <li>TA of EECE 5639 - Computer Vision, Northeastern University, 2018 Fall</li>
    </ul>
  </ul> 
<hr/>


<!--Publications-->
<h2 id="Publications" class="top_main_heading"><span style=:'Times New Roman';">Publications</span></h2>
  <table border="0" width="100%" class="paper" style="border-collapse:separate; border-spacing:15px; margin-left: 2%;">
    <tbody>

    <!--NeRFInvertor22-->
    <tr>
      <td>
        <!-- <img src="./img/NeRFInvertor.png" width="200" height="110" style="margin-right: 15px;"> -->
        <img src="./paper/arXiv2022_NeRFInvertor/NeRFInvertor.png" width="224" height="117" style="margin-right: 15px;">
      </td>
      <td>
        <papertitle><a href="https://arxiv.org/abs/2211.17235">NeRFInvertor: High Fidelity NeRF-GAN Inversion for Single-shot Real Image Animation</a></papertitle> <br><br>
        <strong>Yu Yin</strong>, Kamran Ghasedi, HsiangTao Wu, Jiaolong Yang, Xin Tong, and Yun Fu <br>
        CVPR, 2023 <br>
        <!-- <strong>CVPR Oral</strong> 2023 <br> -->
        <!-- <a href="https://arxiv.org/abs/2211.17235">[arXiv]</a>
        <a href="https://github.com/Picsart-AI-Research/LIVE-Layerwise-Image-Vectorization">[code]</a>
        <a href="https://ma-xu.github.io/LIVE/">[project homepage]</a> -->
        <a data-toggle="modal" href="#abs22NeRFInvertor" class="label label-primary">Abstract</a>
        <a href="https://arxiv.org/abs/2211.17235" class="label label-success">arXiv</a>
        <a href="https://yuyin1.github.io/NeRFInvertor_Homepage/" class="label label-info">Webpage</a>
        <a href="https://youtu.be/va_wrwLB-2k" class="label label-default">Demo</a>
        <a href="https://github.com/YuYin1/NeRFInvertor" class="label label-warning">Code</a>
            <div class="modal fade" id="abs22NeRFInvertor" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                    <h4 class="modal-title" style="text-align: center"><strong>NeRFInvertor: High Fidelity NeRF-GAN Inversion for Single-shot Real Image Animation</strong></h4>
                  </div>
                  <div class="modal-body">
                      <div align="center" style="padding: 0 15px 0">
                        <img src='./paper/arXiv2022_NeRFInvertor/frame.png' alt="paper model" style="width: 100%">
                      </div>
                      </br>   
                      <div style="padding: 0 30px 0">
                        Nerf-based Generative models have shown impressive capacity in generating high-quality images with consistent 3D geometry. Despite successful synthesis of fake identity images randomly sampled from latent space, adopting these models for generating face images of real subjects is still a challenging task due to its so-called inversion issue. In this paper, we propose a universal method to surgically fine-tune these NeRF-GAN models in order to achieve high-fidelity animation of real subjects only by a single image. Given the optimized latent code for an out-of-domain real image, we employ 2D loss functions on the rendered image to reduce the identity gap. Furthermore, our method leverages explicit and implicit 3D regularizations using the in-domain neighborhood samples around the optimized latent code to remove geometrical and visual artifacts. Our experiments confirm the effectiveness of our method in realistic, high-fidelity, and 3D consistent animation of real faces on multiple NeRF-GAN models across different datasets.
                      </div>
                  </div>
            </div>

        <br><br>
      </td>
    </tr>

    <!--FaceEditing22-->
    <tr>
      <td>
        <img src="./paper/arXiv2022_FaceEditing/FaceEditing.png" width="224" height="134" style="margin-right: 15px;">
      </td>
      <td>
        <papertitle><a href="https://arxiv.org/abs/2204.12530">Expanding the Latent Space of StyleGAN for Real Face Editing</a></papertitle> <br><br>
        <strong>Yu Yin</strong>, Kamran Ghasedi, HsiangTao Wu, Jiaolong Yang, Xin Tong, and Yun Fu <br>
        Under review, 2022 <br>
        <a data-toggle="modal" href="#abs22faceedit" class="label label-primary">Abstract</a>
        <a href="https://arxiv.org/abs/2204.12530" class="label label-success">arXiv</a>
            <div class="modal fade" id="abs22faceedit" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                    <h4 class="modal-title" style="text-align: center"><strong>Expanding the Latent Space of StyleGAN for Real Face Editing</strong></h4>
                  </div>
                  <div class="modal-body">
                      <div align="center" style="padding: 0 15px 0">
                        <img src='./paper/arXiv2022_FaceEditing/frame.png' alt="paper model" style="width: 100%">
                      </div>
                      </br>   
                      <div style="padding: 0 30px 0">
                        Recently, a surge of face editing techniques have been proposed to employ the pretrained StyleGAN for semantic manipulation. To successfully edit a real image, one must first convert the input image into StyleGAN’s latent variables. However, it is still challenging to find latent variables, which have the capacity for preserving the appearance of the input subject (\emph{e.g.} identity, lighting, hairstyles) as well as enabling meaningful manipulations. In this paper, we present a method to expand the latent space of StyleGAN with additional content features to break down the trade-off between low-distortion and high-editability. Specifically, we proposed a two-branch model, where the style branch first tackles the entanglement issue by the sparse manipulation of latent codes, and the content branch then mitigates the distortion issue by leveraging the content and appearance details from the input image. We confirm the effectiveness of our method using extensive qualitative and quantitative experiments on real face editing and reconstruction tasks. 
                      </div>
                  </div>
            </div>

        <br><br>
      </td>
    </tr>

    <!--DeepFakeDetection22-->
    <tr>
      <td>
        <img src="./paper/arXiv2022deepfakedetection/deepfakedetection.png" width="224" height="119" style="margin-right: 15px;">
      </td>
      <td>
        <papertitle><a href=" ">Concentric Ring Loss for Face Forgery Detection</a></papertitle> <br><br>
        <strong>Yu Yin</strong>, Yue Bai, and Yun Fu <br>
        Under review, 2022 <br>
        <a data-toggle="modal" href="#abs22deepfakedetection" class="label label-primary">Abstract</a>
        <a href=" " class="label label-success">arXiv</a>
            <div class="modal fade" id="abs22deepfakedetection" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                    <h4 class="modal-title" style="text-align: center"><strong>Concentric Ring Loss for Face Forgery Detection</strong></h4>
                  </div>
                  <div class="modal-body">
                      <div align="center" style="padding: 0 15px 0">
                        <img src='./paper/arXiv2022deepfakedetection/frame.png' alt="paper model" style="width: 100%">
                      </div>
                      </br>   
                      <div style="padding: 0 30px 0">
                        Due to growing societal concerns about indistinguishable deepfake images, face forgery detection has received an increasing amount of interest in computer vi- sion. Since the differences between actual and fake images are frequently small, improving the discriminative ability of learnt features is one of the primary prob- lems in deepfake detection. In this paper, we propose a novel Concentric Ring Loss (CRL) to encourage the model to learn intra-class compressed and inter-class separated features. Specifically, we independently add margin penalties in angu- lar and Euclidean space to force a more significant margin between real and fake images, and hence encourage better discriminating performance. Compared to softmax loss, CRL explicitly encourages intra-class compactness and inter-class separability. Extensive experiments demonstrate the superiority of our methods over multiple datasets. We show that CRL consistently outperforms the state-of- the-art by a large margin. 
                      </div>
                  </div>
            </div>

        <br><br>
      </td>
    </tr>

    <!--InbedPose22-->
    <tr>
      <td>
        <img src="./paper/ACMMM2022_InbedPose/frame.png" width="224" height="130" style="margin-right: 15px;">
      </td>
      <td>
        <papertitle><a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548063">Multimodal In-bed 3D Pose and Shape Estimation under the Blankets</a></papertitle> <br><br>
        <strong>Yu Yin</strong>, Joseph P. Robinson, and Yun Fu <br>
        <span style="font-style: italic;"> ACM International Conference on Multimedia</span> (<strong>ACM MM</strong>), 2022 <br>
        <a data-toggle="modal" href="#abs22inbedpose" class="label label-primary">Abstract</a>
        <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548063" class="label label-success">Paper</a>
        <!-- <a href="https://github.com/YuYin1/SuperFront" class="label label-warning">Code</a> -->
            <div class="modal fade" id="abs22inbedpose" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                    <h4 class="modal-title" style="text-align: center"><strong>Multimodal In-bed 3D Pose and Shape Estimation under the Blankets</strong></h4>
                  </div>
                  <div class="modal-body">
                      <div align="center" style="padding: 0 15px 0">
                        <img src='./paper/ACMMM2022_InbedPose/abstract.png' alt="paper model" style="width: 100%">
                      </div>
                      </br>   
                      <div style="padding: 0 30px 0">
                        Advancing technology to monitor our bodies and behavior while sleeping and resting are essential for healthcare. However, keen challenges arise from our tendency to rest under blankets. We present a multimodal approach to uncover the subjects and view bodies at rest without the blankets obscuring the view. For this, we introduce a channel-based fusion scheme to effectively fuse differ- ent modalities in a way that best leverages the knowledge captured by the multimodal sensors, including visual- and non-visual-based. The channel-based fusion scheme enhances the model’s flexibility in the input at inference: one-to-many input modalities required at test time. Nonetheless, multimodal data or not, detecting humans at rest in bed is still a challenge due to the extreme occlusion when covered by a blanket. To mitigate the negative effects of blanket occlusion, we use an attention-based reconstruction module to explicitly reduce the uncertainty of occluded parts by generating uncovered modalities, which further update the current estimation via a cyclic fashion. Extensive experiments validate the proposed model’s superiority over others.
                      </div>
                  </div>
            </div>
        <br><br>
      </td>
    </tr>

    <!--Floorplans22-->
    <tr>
      <td>
        <img src="./paper/ICMR2022_Floorplans/frame.png" width="224" height="130" style="margin-right: 15px;">
      </td>
      <td>
        <papertitle><a href="https://dl.acm.org/doi/abs/10.1145/3512527.3531384">Generating Topological Structure of Floorplans from Room Attributes</a></papertitle> <br><br>
        <strong>Yu Yin</strong>, Will Hutchcroft, Naji Khosravan, Ivaylo Boyadzhiev, Yun Fu, and Sing Bing Kang <br>
        <span style="font-style: italic;"> ACM International Conference on Multimedia Retrieval</span> (<strong>ACM ICMR</strong>), 2022 <br>
        <a data-toggle="modal" href="#abs22floorplans" class="label label-primary">Abstract</a>
        <a href="https://dl.acm.org/doi/abs/10.1145/3512527.3531384" class="label label-success">Paper</a>
        <!-- <a href="https://github.com/YuYin1/SuperFront" class="label label-warning">Code</a> -->
        <a href="https://youtu.be/q06LZ1taVu4" class="label label-default">Presentation</a>
            <div class="modal fade" id="abs22floorplans" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                    <h4 class="modal-title" style="text-align: center"><strong>Generating Topological Structure of Floorplans from Room Attributes</strong></h4>
                  </div>
                  <div class="modal-body">
                      <div align="center" style="padding: 0 15px 0">
                        <img src='./paper/ICMR2022_Floorplans/frame.png' alt="paper model" style="width: 100%">
                      </div>
                      </br>   
                      <div style="padding: 0 30px 0">
                        Analysis of indoor spaces requires topological information. In this paper, we propose to extract topological information from room attributes using what we call Iterative and adaptive graph Topol- ogy Learning (ITL). ITL progressively predicts multiple relations between rooms; at each iteration, it improves node embeddings, which in turn facilitates generation of a better topological graph structure. This notion of iterative improvement of node embeddings and topological graph structure is in the same spirit as [5]. However, while [5] computes the adjacency matrix based on node similarity, we learn the graph metric using a relational decoder to extract room correlations. Experiments using a new challenging indoor dataset validate our proposed method. Qualitative and quantitative evaluation for layout topology prediction and floorplan generation applications also demonstrate the effectiveness of ITL.
                      </div>
                  </div>
            </div>
        <br><br>
      </td>
    </tr>

    <!--CanSemiSupervised22-->
    <tr>
      <td>
        <img src="./paper/TIP2022_CanSemiSupervised/frame.png" width="224" height="119" style="margin-right: 15px;">
      </td>
      <td>
        <papertitle><a href="https://ieeexplore.ieee.org/document/9944689">Semi-supervised Domain Adaptive Structure Learning</a></papertitle> <br><br>
        Can Qin, Lichen Wang, Qianqian Ma, <strong>Yu Yin</strong>, Huan Wang, and Yun Fu <br>
        <span style="font-style: italic;"> IEEE Transactions on Image Processing</span> (<strong>TIP</strong>), 2022 <br>
        <a data-toggle="modal" href="#abs22CanSemiSupervised" class="label label-primary">Abstract</a>
        <a href="https://ieeexplore.ieee.org/document/9944689" class="label label-success">Paper</a>
        <a href="https://github.com/canqin001/ASDA" class="label label-warning">Code</a>
            <div class="modal fade" id="abs22CanSemiSupervised" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                    <h4 class="modal-title" style="text-align: center"><strong>Semi-supervised Domain Adaptive Structure Learning</strong></h4>
                  </div>
                  <div class="modal-body">
                      <div align="center" style="padding: 0 15px 0">
                        <img src='./paper/TIP2022_CanSemiSupervised/frame.png' alt="paper model" style="width: 100%">
                      </div>
                      </br>   
                      <div style="padding: 0 30px 0">
                        Semi-supervised domain adaptation (SSDA) is quite a challenging problem requiring methods to overcome both 1) overfitting towards poorly annotated data and 2) distribution shift across domains. Unfortunately, a simple combination of domain adaptation (DA) and semi-supervised learning (SSL) methods often fail to address such two objects because of training data bias towards labeled samples. In this paper, we introduce an adaptive structure learning method to regularize the cooperation of SSL and DA. Inspired by the multi-views learning, our proposed framework is composed of a shared feature encoder network and two classifier networks, trained for contradictory purposes. Among them, one of the classifiers is applied to group target features to improve intra-class density, enlarging the gap of categorical clusters for robust representation learning. Meanwhile, the other classifier, serviced as a regularizer, attempts to scatter the source features to enhance the smoothness of the decision boundary. The iterations of target clustering and source expansion make the target features being well-enclosed inside the dilated boundary of the corresponding source points. For the joint address of cross-domain features alignment and partially labeled data learning, we apply the maximum mean discrepancy (MMD) distance minimization and self-training (ST) to project the contradictory structures into a shared view to make the reliable final decision. The experimental results over the standard SSDA benchmarks, including DomainNet and Office- home, demonstrate both the accuracy and robustness of our method over the state-of-the-art approaches.
                      </div>
                  </div>
            </div>

        <br><br>
      </td>
    </tr>

    <!--YueHumanMotion22-->
    <tr>
      <td>
        <img src="./paper/TIP2022_YueHumanMotion/frame.png" width="224" height="119" style="margin-right: 15px;">
      </td>
      <td>
        <papertitle><a href=" ">Human Motion Segmentation via Velocity-Sensitive Dual-Side Auto-Encoder</a></papertitle> <br><br>
        Yue Bai, Lichen Wang, Yunyu Liu, <strong>Yu Yin</strong>, Hang Di, and Yun Fu <br>
        <span style="font-style: italic;"> IEEE Transactions on Image Processing</span> (<strong>TIP</strong>), 2022 <br>
        <a data-toggle="modal" href="#abs22YueHumanMotion" class="label label-primary">Abstract</a>
        <a href="https://ieeexplore.ieee.org/document/9997550" class="label label-success">Paper</a>
        <!-- <a href="https://github.com/canqin001/ASDA" class="label label-warning">Code</a> -->
            <div class="modal fade" id="abs22YueHumanMotion" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                    <h4 class="modal-title" style="text-align: center"><strong>Human Motion Segmentation via Velocity-Sensitive Dual-Side Auto-Encoder</strong></h4>
                  </div>
                  <div class="modal-body">
                      <div align="center" style="padding: 0 15px 0">
                        <img src='./paper/TIP2022_YueHumanMotion/frame.png' alt="paper model" style="width: 100%">
                      </div>
                      </br>   
                      <div style="padding: 0 30px 0">
                        
                      </div>
                  </div>
            </div>

        <br><br>
      </td>
    </tr>

    <!--YueCollaborative22-->
    <tr>
      <td>
        <img src="./paper/SDM2022YueCollaborative/frame.png" width="224" height="145" style="margin-right: 15px;">
      </td>
      <td>
        <papertitle><a href="https://epubs.siam.org/doi/pdf/10.1137/1.9781611977172.56">Collaborative Attention Mechanism for Multi-Modal Time Series Classification</a></papertitle> <br><br>
        Yue Bai, Zhiqiang Tao, Lichen Wang, Sheng Li, <strong>Yu Yin</strong>, and Yun Fu <br>
        <span style="font-style: italic;"> SIAM International Conference on Data Mining</span> (<strong>SDM</strong>), 2022 <br>
        <a data-toggle="modal" href="#abs22yueollaborative" class="label label-primary">Abstract</a>
        <a href="https://epubs.siam.org/doi/pdf/10.1137/1.9781611977172.56" class="label label-success">Paper</a>
            <div class="modal fade" id="abs22yueollaborative" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                    <h4 class="modal-title" style="text-align: center"><strong>Collaborative Attention Mechanism for Multi-Modal Time Series Classification</strong></h4>
                  </div>
                  <div class="modal-body">
                      <div align="center" style="padding: 0 15px 0">
                        <img src='./paper/SDM2022YueCollaborative/frame.png' alt="paper model" style="width: 100%">
                      </div>
                      </br>   
                      <div style="padding: 0 30px 0">
                        Multi-modal time series classification (MTC) uses comple- mentary information from different modalities to improve the learning performance. Obtaining informative modality- specific representation plays an essential role in MTC. At- tention mechanism has been widely adopted as an effective strategy for discovering discriminative cues underlying tem- poral data. However, most existing MTC methods only uti- lize attention to balance the feature weights within or cross modalities but ignore digging latent patterns from mutual- support information in attention space. Specifically, the attention distributions are different for multiple modalities which are supportive and instructional with each other. To this end, we propose a collaborative attention mechanism (CAM) for MTC based on a novel perspective to utilize attention module. CAM detects the attention differences among multi-modal time series, and adaptively integrates different attention information to benefit each other. We extend the long short-term memory (LSTM) to a Mutual-Aid RNN (MAR) for multi-modal collaboration. CAM takes advantages of modality-specific attention to guide another modality and discover potential information which is hard to be explored by itself. It paves a novel way of employing attention to enhance the capacity of multi-modal represen- tations. Extensive experiments on four multi-modal time series datasets illustrate the CAM effectiveness to improve the single-modal and also boost multi-modal performances.
                      </div>
                  </div>
            </div>
        <br><br>
      </td>
    </tr>

    <!--SuperFront21-->
    <tr>
      <td>
        <img src="./paper/ACMMM2021_SuperFront/frame.png" width="224" height="145" style="margin-right: 15px;">
      </td>
      <td>
        <papertitle><a href="https://dl.acm.org/doi/10.1145/3474085.3475300">SuperFront: From Low-resolution Image to High-resolution Frontal Face Sythesis</a></papertitle> <br><br>
        <strong>Yu Yin</strong>, Joseph P. Robinson, Songyao Jiang, Yue Bai, Can Qin, and Yun Fu <br>
        <span style="font-style: italic;"> ACM International Conference on Multimedia</span> (<strong>ACM MM</strong>), 2021 <br>
        <a data-toggle="modal" href="#abs22superfront" class="label label-primary">Abstract</a>
        <a href="https://dl.acm.org/doi/10.1145/3474085.3475300" class="label label-success">Paper</a>
        <a href="https://github.com/YuYin1/SuperFront" class="label label-warning">Code</a>
        <a href="./paper/ACMMM2021_SuperFront/Supplements.pdf" class="label label-default">Supplements</a>
            <div class="modal fade" id="abs22superfront" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                    <h4 class="modal-title" style="text-align: center"><strong>SuperFront: From Low-resolution Image to High-resolution Frontal Face Sythesis</strong></h4>
                  </div>
                  <div class="modal-body">
                      <div align="center" style="padding: 0 15px 0">
                        <img src='./paper/ACMMM2021_SuperFront/frame.png' alt="paper model" style="width: 100%">
                      </div>
                      </br>   
                      <div style="padding: 0 30px 0">
                        Even the most impressive achievement in frontal face synthesis is challenged by large poses and low-quality data given one single side-view face. We propose a synthesizer called SuperFront GAN (SF-GAN) to accept one or more low-resolution (LR) faces at the input to then output a high-resolution (HR) frontal face with various poses and such to preserve identity information. SF-GAN includes intra-class and inter-class constraints, which allow it to learn an identity-preserving representation from multiple LR faces in an improved, comprehensive manner. We adopt an orthogonal loss as the intra-class constraint that diversifies the learned feature-space per subject. Hence, each sample is made to complement the others to its max ability. Additionally, a triplet loss is used as the inter-class constraint: it improves the discriminative power of the new representation, which, hence, maintains the identity information. Furthermore, we integrate a super-resolution (SR) side-view module as part of the SF-GAN to help preserve the finer details of HR side-views. This helps the model reconstruct the high-frequency parts of the face (i.e. periocular region, nose, and mouth regions). Quantitative and qualitative results demonstrate the superiority of SF-GAN. SF-GAN holds promise as a pre-processing step to normalize and align faces before passing to CV system for processing.
                      </div>
                  </div>
            </div>

        <br><br>
      </td>
    </tr>

    <!--CanSemiSupervised21-->
    <tr>
      <td>
        <img src="./paper/TIP2022_CanSemiSupervised/frame2.png" width="224" height="110" style="margin-right: 15px;">
      </td>
      <td>
        <papertitle><a href="https://arxiv.org/pdf/2002.02545.pdf">Opposite Structure Learning for Semi-supervised Domain Adaptation</a></papertitle> <br><br>
        Can Qin, Lichen Wang, Qianqian Ma, <strong>Yu Yin</strong>, Huan Wang, and Yun Fu <br>
        <span style="font-style: italic;"> SIAM International Conference on Data Mining</span> (<strong>SDM</strong>), 2021 <br>
        <a data-toggle="modal" href="#abs21CanSemiSupervised" class="label label-primary">Abstract</a>
        <a href="https://arxiv.org/pdf/2002.02545.pdf" class="label label-success">Paper</a>
        <a href="https://github.com/canqin001/Contradictory-Structure-Learning-for-Semi-supervised-Domain-Adaptation" class="label label-warning">Code</a>
            <div class="modal fade" id="abs21CanSemiSupervised" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                    <h4 class="modal-title" style="text-align: center"><strong>Opposite Structure Learning for Semi-supervised Domain Adaptation</strong></h4>
                  </div>
                  <div class="modal-body">
                      <div align="center" style="padding: 0 15px 0">
                        <img src='./paper/TIP2022_CanSemiSupervised/frame.png' alt="paper model" style="width: 100%">
                      </div>
                      </br>   
                      <div style="padding: 0 30px 0">
                        Current adversarial adaptation methods attempt to align the cross-domain features, whereas two challenges remain unsolved: 1) the conditional distribution mismatch and 2) the bias of the decision boundary towards the source domain. To solve these challenges, we propose a novel framework for semi-supervised domain adaptation by unifying the learning of opposite structures (UODA). UODA consists of a generator and two classifiers (i.e., the sourcescattering classifier and the target-clustering classifier), which are trained for contradictory purposes. The target-clustering classifier attempts to cluster the target features to improve intra-class density and enlarge inter-class divergence. Meanwhile, the source-scattering classifier is designed to scatter the source features to enhance the decision boundary’s smoothness. Through the alternation of source-feature expansion and target-feature clustering procedures, the target features are well-enclosed within the dilated boundary of the corresponding source features. This strategy can make the cross-domain features to be precisely aligned against the source bias simultaneously. Moreover, to overcome the model collapse through training, we progressively update the measurement of feature’s distance and their representation via an adversarial training paradigm. Extensive experiments on the benchmarks of DomainNet and Office-home datasets demonstrate the superiority of our approach over the state-of-the-art methods.
                      </div>
                  </div>
            </div>

        <br><br>
      </td>
    </tr>

    <!--DualGAN20-->
    <tr>
      <td>
        <img src="./paper/FG2021_DualGAN/frame.png" width="224" height="129" style="margin-right: 15px;">
      </td>
      <td>
        <papertitle><a href="https://www.computer.org/csdl/proceedings-article/fg/2020/307900a024/1kecHPwIBLa">Dual-Attention GAN for Large-Pose Face Frontalization</a></papertitle> <br><br>
        <strong>Yu Yin</strong>, Songyao Jiang, Joseph P. Robinson, and Yun Fu <br>
        <span style="font-style: italic;"> IEEE International Conference on Automatic Face and Gesture Recognition</span> (<strong>FG</strong>), 2020 <br>
        <a data-toggle="modal" href="#abs21dualgan" class="label label-primary">Abstract</a>
        <a href="https://www.computer.org/csdl/proceedings-article/fg/2020/307900a024/1kecHPwIBLa" class="label label-success">Paper</a>
        <a href="https://arxiv.org/abs/2002.07227" class="label label-success">Paper</a>
        <a href="https://github.com/YuYin1/DA-GAN" class="label label-warning">Code</a>
        <a href="https://youtu.be/r-V3IwZCdDw" class="label label-default">Presentation</a>
            <div class="modal fade" id="abs21dualgan" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                    <h4 class="modal-title" style="text-align: center"><strong>Dual-Attention GAN for Large-Pose Face Frontalization</strong></h4>
                  </div>
                  <div class="modal-body">
                      <div align="center" style="padding: 0 15px 0">
                        <img src='./paper/FG2021_DualGAN/abstract.png' alt="paper model" style="width: 100%">
                      </div>
                      </br>   
                      <div style="padding: 0 30px 0">
                        Face frontalization provides an effective and effi- cient way for face data augmentation and further improves the face recognition performance in extreme pose scenario. Despite recent advances in deep learning-based face synthesis ap- proaches, this problem is still challenging due to significant pose and illumination discrepancy. In this paper, we present a novel Dual-Attention Generative Adversarial Network (DA-GAN) for photo-realistic face frontalization by capturing both contextual dependencies and local consistency during GAN training. Specifically, a self-attention-based generator is introduced to integrate local features with their long-range dependencies yielding better feature representations, and hence generate faces that preserves identities better, especially for larger pose angles. Moreover, a novel face-attention-based discriminator is applied to emphasize local features of face regions, and hence reinforce the realism of synthetic frontal faces. Guided by semantic segmentation, four independent discriminators are used to distinguish between different aspects of a face (i.e., skin, keypoints, hairline, and frontalized face). By introducing these two complementary attention mechanisms in generator and discriminator separately, we can learn a richer feature representation and generate identity preserving inference of frontal views with much finer details (i.e., more accurate facial appearance and textures) comparing to the state-of-the-art. Quantitative and qualitative experimental results demonstrate the effectiveness and efficiency of our DA-GAN approach.
                      </div>
                  </div>
            </div>

        <br><br>
      </td>
    </tr>

    <!--JointAliSR20-->
    <tr>
      <td>
        <img src="./paper/AAAI2020_JointAliSR/frame.png" width="224" height="140" style="margin-right: 15px;">
      </td>
      <td>
        <papertitle><a href="https://arxiv.org/abs/1911.08566">Joint Super-Resolution and Alignment of Tiny Faces</a></papertitle> <br><br>
        <strong>Yu Yin</strong>, Joseph P. Robinson, Yulun Zhang, and Yun Fu <br>
        <span style="font-style: italic;"> AAAI Conference on Artificial Intelligence</span> (<strong>AAAI</strong>), 2020 <br>
        <a data-toggle="modal" href="#abs21jointalisr" class="label label-primary">Abstract</a>
        <a href="https://arxiv.org/abs/1911.08566" class="label label-success">Paper</a>
        <a href="https://github.com/YuYin1/JASRNet" class="label label-warning">Code</a>
            <div class="modal fade" id="abs21jointalisr" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                    <h4 class="modal-title" style="text-align: center"><strong>Joint Super-Resolution and Alignment of Tiny Faces</strong></h4>
                  </div>
                  <div class="modal-body">
                      <div align="center" style="padding: 0 15px 0">
                        <img src='./paper/AAAI2020_JointAliSR/frame.png' alt="paper model" style="width: 100%">
                      </div>
                      </br>   
                      <div style="padding: 0 30px 0">
                        Super-resolution (SR) and landmark localization of tiny faces are highly correlated tasks. On the one hand, landmark lo- calization could obtain higher accuracy with faces of high- resolution (HR). On the other hand, face SR would benefit from prior knowledge of facial attributes such as landmarks. Thus, we propose a joint alignment and SR network to si- multaneously detect facial landmarks and super-resolve tiny faces. More specifically, a shared deep encoder is applied to extract features for both tasks by leveraging complementary information. To exploit representative power of the hierarchi- cal encoder, intermediate layers of a shared feature extraction module are fused to form efficient feature representations. The fused features are then fed to task-specific modules to de- tect landmarks and super-resolve face images in parallel. Ex- tensive experiments demonstrate that the proposed model sig- nificantly outperforms the state-of-the-art in both landmark localization and SR of faces. We show a large improvement for landmark localization of tiny faces (i.e., 16 × 16). Fur- thermore, the proposed framework yields comparable results for landmark localization on low-resolution (LR) faces (i.e., 64 × 64) to existing methods on HR (i.e., 256 × 256). As for SR, the proposed method recovers sharper edges and more details from LR face images than other state-of-the-art meth- ods, which we demonstrate qualitatively and quantitatively.
                      </div>
                  </div>
            </div>
        <br><br>
      </td>
    </tr>

    <!--YueHumanMotion20-->
    <tr>
      <td>
        <img src="./paper/TIP2022_YueHumanMotion/frame2.gif" width="224" height="119" style="margin-right: 15px;">
      </td>
      <td>
        <papertitle><a href="https://ieeexplore.ieee.org/document/9338296">Dual-Side Auto-Encoder for High-Dimensional Time Series Segmentation</a></papertitle> <br><br>
        Yue Bai, Lichen Wang, Yunyu Liu, <strong>Yu Yin</strong>, and Yun Fu <br>
        <span style="font-style: italic;"> IEEE International Conference on Data Mining</span> (<strong>ICDM</strong>), 2020 <br>
        <a data-toggle="modal" href="#abs20YueHumanMotion" class="label label-primary">Abstract</a>
        <a href="https://ieeexplore.ieee.org/document/9338296" class="label label-success">Paper</a>
            <div class="modal fade" id="abs20YueHumanMotion" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                    <h4 class="modal-title" style="text-align: center"><strong>Dual-Side Auto-Encoder for High-Dimensional Time Series Segmentation</strong></h4>
                  </div>
                  <div class="modal-body">
                      <div align="center" style="padding: 0 15px 0">
                        <img src='./paper/TIP2022_YueHumanMotion/frame.png' alt="paper model" style="width: 100%">
                      </div>
                      </br>   
                      <div style="padding: 0 30px 0">
                        High-dimensional time series segmentation aims to segment a long temporal sequence into several short and meaningful subsequences. The high-dimensionality makes it challenging due to the complicated correlations among the sequential features. A large number of labeled data is required in existing supervised methods, and unsupervised methods mainly deploy clustering approaches, which are sensitive to outliers and hard to guarantee high performance. Also, most existing methods mainly rely on hand-craft features to deal with regular time series segmentation and achieve promising results. However, these approaches cannot effectively handle high-dimensional time series and will result in a high computational cost. In our work, we propose a novel unsupervised representation learning framework called Dual-Side Auto-Encoder (DSAE). It mainly focuses on high-dimensional time series segmentation by effectively capturing the temporal correlative patterns. Specifically, a single-to-multiple auto-encoder is designed to capture local sequential information. Besides, a long-shot distance encoding strategy is proposed. It aims to explicitly guide the learning process to obtain distinctive representations for segmentation. Furthermore, the long-short distance strategy is also executed in the decoded feature space, which implicitly directs the representation learning. Substantial experiments on six datasets illustrate the model effectiveness.
                      </div>
                  </div>
            </div>

        <br><br>
      </td>
    </tr>

    <!--Affective18-->
    <tr>
      <td>
        <img src="./paper/ijcai2018_expression/frame2.jpg" width="224" height="102" style="margin-right: 15px;">
      </td>
      <td>
        <papertitle><a href="https://arxiv.org/abs/1811.07392">Facial Expression and Peripheral Physiology Fusion to Decode Individualized Affective Experience</a></papertitle> <br><br>
        <strong>Yu Yin</strong>, Mohsen Nabian, Miolin Fan, ChunAn Chou, Maria Gendron, and Sarah Ostadabbas <br>
        <span style="font-style: italic;"> Affective Computing Workshop of the International Joint Conferences on Artificial Intelligence</span> (<strong>IJCAI Workshop</strong>), 2018 <br>
        <a data-toggle="modal" href="#abs18facial" class="label label-primary">Abstract</a>
        <a href="https://arxiv.org/abs/1811.07392" class="label label-success">arXiv</a>
        <a href="https://github.com/ostadabbas/3d-facial-landmark-detection-and-tracking" class="label label-warning">Code</a>
        <a href="https://youtu.be/3C7d8fecIhM" class="label label-default">Demo</a>
        <a href="./paper/ijcai2018_expression/AffCom_IJCAI2018_Presentation.pdf" class="label label-info">Slides</a>
            <div class="modal fade" id="abs18facial" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                    <h4 class="modal-title" style="text-align: center"><strong>Facial Expression and Peripheral Physiology Fusion to Decode Individualized Affective Experience</strong></h4>
                  </div>
                  <div class="modal-body">
                      <div align="center" style="padding: 0 15px 0">
                        <img src='./paper/ijcai2018_expression/frame.png' alt="paper model" style="width: 100%">
                      </div>
                      </br>   
                      <div style="padding: 0 30px 0">
                        In this paper, we present a multimodal approach to simultaneously analyze facial movements and several peripheral physiological signals to decode individualized affective experiences under positive and negative emotional contexts, while considering their personalized resting dynamics. We propose a person-specific recurrence network to quantify the dynamics present in the person's facial movements and physiological data. Facial movement is represented using a robust head vs. 3D face landmark localization and tracking approach, and physiological data are processed by extracting known attributes related to the underlying affective experience. The dynamical coupling between different input modalities is then assessed through the extraction of several complex recurrent network metrics. Inference models are then trained using these metrics as features to predict individual's affective experience in a given context, after their resting dynamics are excluded from their response. We validated our approach using a multimodal dataset consists of (i) facial videos and (ii) several peripheral physiological signals, synchronously recorded from 12 participants while watching 4 emotion-eliciting video-based stimuli. The affective experience prediction results signified that our multimodal fusion method improves the prediction accuracy up to 19% when compared to the prediction using only one or a subset of the input modalities. Furthermore, we gained prediction improvement for affective experience by considering the effect of individualized resting dynamics.
                      </div>
                  </div>
            </div>
        <br><br>
      </td>
    </tr>

    <!--BiosignalTool17-->
    <tr>
      <td>
        <img src="./paper/hipoct2017_bioTool/frame2.png" width="224" height="120" style="margin-right: 15px;">
      </td>
      <td>
        <papertitle><a href="https://ieeexplore.ieee.org/document/8227588">A Biosignal-Specific Processing Tool for Machine Learning and Pattern Recognition</a></papertitle> <br><br>
        <strong>Yu Yin*</strong>, Mohsen Nabian*, Athena Nouhi*, and Sarah Ostadabbas <br>(* equal contribution)<br> 
        <span style="font-style: italic;">IEEE-NIH Special Topics Conference on Healthcare Innovations and Point-of-Care Technologies</span> (<strong>HI-POCT</strong>), 2017 <br>
        <a data-toggle="modal" href="#abs17biosignal" class="label label-primary">Abstract</a>     
        <a href="./paper/hipoct2017_bioTool/HI_POCT_2017_Yu.pdf" class="label label-success">PDF</a>
        <a href="https://www.mathworks.com/matlabcentral/fileexchange/64013-biosignal-specific-processing-bio-sp-tool" class="label label-warning">Matlab Code</a>
        <a href="./paper/hipoct2017_bioTool/Poster_HI_POCT_2017.pdf" class="label label-info">Poster</a>
            <div class="modal fade" id="abs17biosignal" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                    <h4 class="modal-title" style="text-align: center"><strong>A Biosignal-Specific Processing Tool for Machine Learning and Pattern Recognition.</strong></h4>
                  </div>
                  <div class="modal-body">
                      <div align="center" style="padding: 0 15px 0">
                        <img src='./paper/hipoct2017_bioTool/frame.png' alt="paper model" style="width: 100%">
                      </div>
                      </br>   
                      <div style="padding: 0 30px 0">
                        Electrocardiogram (ECG), Electrodermal Activity (EDA), Electromyogram (EMG) and Impedance Cardiography (ICG) are among physiological signals widely used in various biomedical applications including health tracking, sleep quality assessment, early disease detection/diagnosis and human affective state recognition. This paper presents the development of a biosignal-specific processing and feature extraction tool for analyzing these physiological signals according to the state-ofthe-art studies reported in the scientific literature. This tool is intended to assist researchers in machine learning and pattern recognition to extract feature matrix from these bio-signals automatically and reliably. In this paper, we provided the algorithms used for the signal-specific filtering and segmentation as well as extracting features that have been shown highly relevant to a better category discrimination in an intended application. This tool is an open-source software written in MATLAB and made compatible with MathWorks Classification Learner app for further classification purposes such as model training, cross-validation scheme farming, and classification result computation.
                      </div>
                  </div>
                </div>
            </div>
        <br><br>
      </td>
    </tr>

    <!--JTEHM_Shuangjun19-->
    <tr>
      <td>
        <img src="./paper/jiehm2019_pose/frame.jpg" width="224" height="120" style="margin-right: 15px;">
      </td>
      <td>
      <papertitle><a href="https://arxiv.org/abs/1711.01005">In-Bed Pose Estimation: Deep Learning with Shallow Dataset</a></papertitle> <br><br>
      Shuangjun Liu, <strong>Yu Yin</strong>, and Sarah Ostadabbas <br>
      <span style="font-style: italic;">IEEE Journal of Translational Engineering in Health and Medicine</span> (<strong>JTEHM</strong>), 2019 <br>
      <a data-toggle="modal" href="#absjtehm19inbed" class="label label-primary">Abstract</a>
      <a href="https://arxiv.org/abs/1711.01005" class="label label-success">arXiv</a>
      <a href="https://github.com/ostadabbas/in-bed-pose-estimation" class="label label-warning">Code</a>
      <a href="https://web.northeastern.edu/ostadabbas/2017/09/01/a-vision-based-system-for-in-bed-posture-tracking/" class="label label-default">Project</a>
          <div class="modal fade" id="absjtehm19inbed" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
              <div class="modal-content">
                <div class="modal-header">
                  <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                  <h4 class="modal-title" style="text-align: center"><strong>In-Bed Pose Estimation: Deep Learning with Shallow Dataset.</strong></h4>
                </div>
                <div class="modal-body">
                <div align="center" style="padding: 0 15px 0">
                  <img src='./paper/jiehm2019_pose/frame.jpg' alt="paper model" style="width: 100%">
                </div>
                </br>   
                <div style="padding: 0 30px 0">
                  This paper presents a robust human posture and body parts detection method under a specific application scenario, in-bed pose estimation. Although human pose estimation for various computer vision (CV) applications has been studied extensively in the last few decades, yet in-bed pose estimation using camera-based vision methods has been ignored by the CV community because it is assumed to be identical to the general purpose pose estimation methods. However, in-bed pose estimation has its own specialized aspects and comes with specific challenges including the notable differences in lighting conditions throughout a day and also having different pose distribution from the common human surveillance viewpoint. In this paper, we demonstrate that these challenges significantly lessen the effectiveness of existing general purpose pose estimation models. In order to address the lighting variation challenge, infrared selective (IRS) image acquisition technique is proposed to provide uniform quality data under various lighting conditions. In addition, to deal with unconventional pose perspective, a 2-end histogram of oriented gradient (HOG) rectification method is presented. Deep learning framework proves to be the most effective model in human pose estimation, however the lack of large public dataset for in-bed poses prevents us from using a large network from scratch. In this work, we explored the idea of employing a pre-trained convolutional neural network (CNN) model trained on large public datasets of general human poses and fine-tuning the model using our own shallow (limited in size and different in perspective and color) in-bed IRS dataset. We developed an IRS imaging system and collected IRS image data from several realistic life-size mannequins in a simulated hospital room environment. A pre-trained CNN called convolutional pose machine (CPM) was repurposed for in-bed pose estimation by fine-tuning its specific intermediate layers. Using the HOG rectification method, the pose estimation performance of CPM significantly improved by 26.4% in PCK0.1 (probability of correct keypoint) criteria compared to the model without such rectification. Even testing with only well aligned in-bed pose images, our fine-tuned model still surpassed the traditionally-tuned CNN by another 16.6% increase in pose estimation accuracy.
                </div>
            </div>
          </div>
        <br><br>
      </td>
    </tr>


  </tbody></table>

  More previous publications:
  <ul>
    <li>Analysis of Multimodal Physiological Signals Within and Across Individuals to Predict Psychological Threat vs. Challenge</a></li>
    Aya Khalaf, Mohsen Nabian, Miaolin Fan, <b>Yu Yin</b>, Jolie Wormwood, Erika Siegel, Karen S. Quigley, Lisa Feldman Barrett, Murat Akcakaya, Chun-An Chou, and Sarah Ostadabbas</br>
    <span style="font-style: italic;">Expert Systems With Applications (<b>ESWA</b>), vol. 140, Feb. 2020. </span><br/>
    <a href="https://psyarxiv.com/96djs/" class="label label-success">psyarXiv</a>
    <a data-toggle="modal" href="#absjtehm19inbed" class="label label-primary">Abstract</a>
      <div class="modal fade" id="absjtehm19inbed" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
              <h4 class="modal-title" style="text-align: center"><strong>Analysis of Multimodal Physiological Signals Within and Across Individuals to Predict PsychologicalThreat vs. Challenge.</strong></h4>
            </div>
            <div class="modal-body">
                <div align="center" style="padding: 0 15px 0">
                  <img src='./paper/eswa2020_threat/frame.png' alt="paper model" style="width: 100%">
                </div>
                </br>   
                <div style="padding: 0 30px 0">
                  In this study, we aimed to investigate individual and group-level variations in physiological responding across a series of motivated performance tasks that vary in difficulty. The proposed approach is motivated by documented individual differences in physiological responses observed in motivated performance tasks, such that we first focus on individual differences in physiological responses rather than group-level comparisons. Then, through our analysis of individuals we identify sub-groups (i.e., clusters) of individuals that share common physiological patterns across tasks of varying difficulty and we perform across-subject analysis within each cluster. This is distinct from existing studies which typically do not examine individual vs. subgroup-specific patterns of physiological activity. Such an approach enables us to identify patterns in physiological responses that can be used to predict self-reported judgments of challenge vs. threat with higher accuracy in each subgroup compared to an analysis that includes the entire sample population as a single group. We employed data from an existing experiment in which participants completed three mental arithmetic tasks of increasing difficulty during which different modalities of physiological data were collected. Analyses revealed three subgroups of participants who shared common features that best differentiated their within-individual physiological response patterns across tasks. Support vector machine (SVM) classifiers were trained using both shared features within each group and all computed features to predict challenge vs. threat states. Results showed that, the within-group classification model using group common features achieved higher self-report prediction accuracy compared to an alternative model trained on data from all participants without feature selection.
                </div>
            </div>
      </div>
  </ul>

  <ul>
    <li>An Open-Source Feature Extraction Tool for the Analysis of Peripheral Physiological Data</a></li>
    Mohsen Nabian, <b>Yu Yin</b>, Jolie Wormwood, Karen S. Quigley, Lisa F. Barrett, and Sarah Ostadabbas</br>
    <span style="font-style: italic;">IEEE Journal of Translational Engineering in Health and Medicine</span> (<b>JTEHM</b>), vol. 6, Oct. 2019.<br/>
    <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6231905/pdf/jtehm-ostadabbas-2878000.pdf" class="label label-success">PDF</a>
    <a data-toggle="modal" href="#absjtehmbiosp" class="label label-primary">Abstract</a>
    <a href="https://www.mathworks.com/matlabcentral/fileexchange/64013-biosignal-specific-processing-bio-sp-tool" class="label label-warning">Matlab Code</a>
        <div class="modal fade" id="absjtehmbiosp" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
            <div class="modal-content">
              <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
                <h4 class="modal-title" style="text-align: center"><strong>An Open-Source Feature Extraction Tool for the Analysis of Peripheral Physiological Data.</strong></h4>
              </div>
              <div class="modal-body">
                  <div align="center" style="padding: 0 15px 0">
                    <img src='./paper/jiehm2019_bioTool/frame.png' alt="paper model" style="width: 100%">
                  </div>
                  </br>   
                  <div style="padding: 0 30px 0">
                    Electrocardiogram (ECG), Electrodermal Activity (EDA), Electromyogram (EMG), continuous Blood Pressure (BP) and Impedance Cardiography (ICG) are among the physiological signals widely used in various biomedical applications including health tracking, sleep quality assessment, early disease detection/diagnosis, telemedicine and human affective state recognition. This paper presents the development of a biosignal-specific tool for processing and feature extraction of these physiological signals according to state-of-the-art studies reported in the scientific literature and feedback received from field experts. This tool is intended to assist researchers in affective computing, machine learning, and pattern recognition to extract the physiological features from these biosignals automatically and reliably. In this paper, we provide algorithms for signal-specific quality checking, filtering, and segmentation as well as the extraction of features that have been shown to be highly relevant to category discrimination in biomedical and affective computing applications. This tool is an open-source software written in MATLAB and a graphical user interface (GUI) is also provided for the convenience of the users. The GUI is compatible with MathWorks Classification Learner app for further *classification purposes such as model training, cross-validation scheme farming, and classification result computation.
                  </div>
              </div>
        </div>
  </ul>
<hr/>


<!--Patents-->
<h2 id="Patents" class="top_main_heading"><span style=:'Times New Roman';">Patents</span></h2>
  <ul>
    <li>Yun Fu, <b>Yu Yin</b>, “Frontal Face Synthesis from Low-Resolution Images.” U.S. Patent Application No. 17/156,204. </li> 
    <li><b>Yu Yin</b>, Will A. Hutchcroft, Ivaylo Boyadzhiev, Sing Bing Kang, Yujie Li, Pierre Moulon, “Automated Identification And Use Of Building Floor Plan Information.” U.S. Patent Application No. 17/472,527. </li>
  </ul>
<hr/>


<!--Awards-->
<h2 id="Awards" class="top_main_heading"><span style=:'Times New Roman';">Awards</span></h2>
  <ul>
    <li>CVPR Travel Grant, 2023</li>
    <li>Dissertation Completion Fellowship, Northeastern University, USA, 2023</li> 
    <li>SIG MM Travel Grant, 2022</li> 
    <li>NSF I-Corps Grant, 2022</li> 
    <li>AAAI Travel Grant, 2020</li>
    <li>PhD Network Travel Grant, Northeastern University, USA, 2019</li>
    <li>Excellent Bachelor Thesis of Wuhan University of Technology, 2016</li>
    <li>Scholarship of Wuhan University of Technology, 2013, 2014, 2015</li>
  </ul>
<hr/>


<!--Activities-->
<h2 id="Activities" class="top_main_heading"><span style=:'Times New Roman';">Activities</span></h2>
  <ul> 
    <li><strong>Chair and Co-organizer of:</strong>
    <ul> 
      <li> Workshop of Analysis and Modeling of Faces and Gestures <a href="https://web.northeastern.edu/smilelab/amfg2023/">(AMFG'23)</a> (in conjunctionwith ICCV'23) </li>
      <li> Workshop of Analysis and Modeling of Faces and Gestures <a href="https://web.northeastern.edu/smilelab/amfg2021/">(AMFG'21)</a> (in conjunctionwith CVPR'21) </li>
      <li> Recognizing Families In the Wild Challenge <a href="https://web.northeastern.edu/smilelab/rfiw2020/">(RFIW'20)</a> (in conjunction with FG'20) </li>
    </ul>
  </ul> 
  <ul> 
    <li><strong>Reviewers</strong>
    <ul> 
      <li>IEEE Transactions on Image Processing (TIP)</li>
      <li>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</li>
      <li>IEEE Transactions on Cybernetics (TCyber)</li>
      <li>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</li>
      <li>Information Sciences (Elsevier)</li>
      <li>Conferences: CVPR, ECCV, ICCV, IJCAI, AAAI, ACM MM, ICDM, FG</li>
    </ul>
  </ul> 

<hr />


<p><span class="STYLE189 STYLE263">Created on 2019/10/01. Last update : 2023/04/22</span></p>
<div style='margin:1% 30% 1% 30%'>
  <!-- <p>    
  <a href="https://clustrmaps.com/site/1b0h9"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=GiDrPWoG3lMZDBc9YjT9O7Dih0JJe5hgvTCgWflrrzs&cl=ffffff" /></a>
  </p></body></html> -->
  <center>
  <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=GiDrPWoG3lMZDBc9YjT9O7Dih0JJe5hgvTCgWflrrzs&cl=ffffff&w=a"></script>
  </center>
</div>


<br>
<br>



<!--<h3>More Projects</h3>-->

</div> <!-- end main content section -->
</div>

</div> <!-- end container -->
</body>

</html>
